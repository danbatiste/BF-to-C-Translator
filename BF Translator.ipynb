{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARSET = \"<>[]+-.,\"\n",
    "ERROR = \"Error!\"\n",
    "REDUNDANCIES = [\"+-\", \"-+\", \"<>\", \"><\", \"[]\"]\n",
    "\n",
    "def strip_chars(stdio):\n",
    "    global CHARSET\n",
    "    return ''.join([c for c in stdio if c in CHARSET])\n",
    "\n",
    "\n",
    "def remove_redundant(stdio):\n",
    "    global REDUNDANCIES\n",
    "    output = stdio\n",
    "    for redundancy in REDUNDANCIES:\n",
    "        output = output.replace(redundancy, '')\n",
    "    output2 = output\n",
    "    for redundancy in REDUNDANCIES:\n",
    "        output2 = output2.replace(redundancy, '')\n",
    "    if output != output2:\n",
    "        return remove_redundant(output2)\n",
    "    else:\n",
    "        return output2\n",
    "\n",
    "\n",
    "def tokenize(stdio):\n",
    "    ##  \"++++--->>>[++]\"\n",
    "    ##  [ ('+', 1), ('>', 3), ('[', tokenize(\"++\")) ]) ]\n",
    "    ## [ ('+', 1), ('>', 3), ('[', [('+', 2)]) ]) ]\n",
    "    expr = [] \n",
    "    i = 0\n",
    "    while i < len(stdio):\n",
    "        # Set up lastchar and char vars\n",
    "        char = stdio[i]\n",
    "        if i >= 1:\n",
    "            lastchar = stdio[i-1]\n",
    "        else:\n",
    "            lastchar = \"\"\n",
    "    \n",
    "        # Actual tokenization done here\n",
    "        if char in \"+-\":\n",
    "            if (lastchar != \"\") and (lastchar in \"+-\"):\n",
    "                # Add or subtract 1 from last token\n",
    "                expr[-1][1] += eval(f\"{char}1\") # \"+1\" or \"-1\"\n",
    "                i += 1\n",
    "            else:\n",
    "                token = ['+', (-1,1)[char==\"+\"]] #('+', 1) if char==\"+\" else -1\n",
    "                expr.append(token)\n",
    "                i += 1\n",
    "        # Same concept as \"+-\" charset\n",
    "        if char in \"><\":\n",
    "            val = (-1,1)[char==\">\"]\n",
    "            if (lastchar != \"\") and (lastchar in \"><\"):\n",
    "                expr[-1][1] += eval(f\"{val}\")\n",
    "                i += 1\n",
    "            else:\n",
    "                token = ['>', (-1,1)[char==\">\"]]\n",
    "                expr.append(token)\n",
    "                i += 1\n",
    "        # No argument, so token is an str and not a list\n",
    "        if char in '.,':\n",
    "            token = char\n",
    "            expr.append(token)\n",
    "            i += 1   \n",
    "        # Bracket tokenization here\n",
    "        if char == \"]\":\n",
    "            return ERROR\n",
    "        if char == \"[\":\n",
    "            i += 1\n",
    "            depth = 1\n",
    "            temp_expr = ''\n",
    "            while depth != 0:\n",
    "                try:\n",
    "                    char = stdio[i]\n",
    "                except:\n",
    "                    return ERROR\n",
    "                if char == ']' and depth == 0:\n",
    "                    break\n",
    "                if char == '[':\n",
    "                    depth += 1\n",
    "                if char == ']':\n",
    "                    depth -= 1\n",
    "                if depth == 0:\n",
    "                    break\n",
    "                temp_expr += char\n",
    "                i += 1\n",
    "            token = ['[',tokenize(temp_expr)]\n",
    "            expr.append(token)\n",
    "            i += 1 # skips the initial '[' and also\n",
    "                   # the ']' that is the current char\n",
    "    return expr\n",
    "\n",
    "\n",
    "def translate(tokens):\n",
    "    tabulate = lambda line: \" \"*2 + line\n",
    "    char_tokens = {\n",
    "        \".\" : \"putchar(*p);\",\n",
    "        \",\" : \"*p = getchar();\",\n",
    "    }\n",
    "    output_lines = []\n",
    "    for token in tokens:\n",
    "        if type(token) == str:\n",
    "            output_lines.append(char_tokens[token])\n",
    "        if token[0] == '>':\n",
    "            arg = token[1]\n",
    "            sign = ('+','-')[arg<=0]\n",
    "            output_lines.append(f\"p {sign}= {abs(arg)};\")\n",
    "        if token[0] == '+':\n",
    "            arg = token[1]\n",
    "            sign = ('+','-')[arg<=0]\n",
    "            output_lines.append(f\"*p {sign}= {abs(arg)};\")\n",
    "        if token[0] == '[':\n",
    "            arg = token[1]\n",
    "            loop_lines = [\"if (*p) do {\"] +\\\n",
    "                         list(map(tabulate, translate(arg))) +\\\n",
    "                         [\"} while (*p);\"]\n",
    "            output_lines += loop_lines\n",
    "    return output_lines\n",
    "                \n",
    "    \n",
    "def brainfuck_to_c(bf):\n",
    "    formatted_bf = remove_redundant(strip_chars(bf))\n",
    "    tokens = tokenize(formatted_bf)\n",
    "    if tokens == ERROR:\n",
    "        return ERROR\n",
    "    translated_bf = translate(tokens)\n",
    "    compiled = '\\n'.join(translated_bf)\n",
    "    return compiled + ('\\n',\"\")[compiled==\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if (*p) do {\n",
      "  if (*p) do {\n",
      "    putchar(*p);\n",
      "  } while (*p);\n",
      "  *p += 3;\n",
      "} while (*p);\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "bf = \"[[.]+++]\"\n",
    "print(brainfuck_to_c(bf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
